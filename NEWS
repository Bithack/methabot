1.6.0
=====
Here comes another feature-packed  release  of  Methabot.  This release contains
such fine new features as  Javascript-MySQL support, parser chaining support and
better URL filtering with support for robots.txt.

Changes and new features:
* New configuration and  script  for  automatically  validating HTML/XHTML pages
  through W3C's validator
* The builtin HTML parser is now much more fault tolerant.
* Modules can now register configuration file scopes and thus be configured from
  configuration files.
* Modules can register  objects  and  functions  that  will  be available to the
  workers (javascript)
* The load_module directive  is  now  available  so  the  user can manually load
  modules.
* Javascript-MySQL binding support through the new lmm_mysql loadable module.
* New parser system with support for  parser chaining. Parsers are now protocol-
  independent.
* Support for handlers, the 'handler' filetype option, and the 'default_handler'
  crawler option.
* New configuration for searching Wikipedia.
* Lots of multithreading optimization and performance improvements
* New option --jail, -J
* Robots Exclusion Standard support
* New URL filtering system
* Lots of improvements to the hostname caching
* New configuration  xmlsource.conf,  converts  HTML  to  XHTML  and outputs the
  converted data.
* Improvements to piping data in and out of methabot
* New configuration for finding RSS feeds on websites
* Support for extracting URLs from CSS files and HTML style elements
* Bugfix   when   linking    a   javascript    library    that  was  built  with
  MOZILLA_1_8_BRANCH defined

1.5.0
=====
This version contains many improvements, new features and bugfixes. Don't forget
to check out the project's website! 
 
Changes and new features: 
* Support for reading intial buffer from stdin 
* --type and --base-url command line options added, along with the
  initial_filetype option in configuration files 
* Cookies and DNS info is now properly shared between workers when running
  multithreaded 
* Added some example usage commands to --examples 
* Big improvements to the inter-thread communication, now faster and more
  organized 
* Added support for 'init' functions to scripts. Read more about init functions
  at http://bithack.se/projects/methabot/docs/e4x/init_functions.html 
* libmetha doesn't freeze when doing multiple concurrent HTTP HEAD requests 
  anymore. The reason for the freezes was a bug in libcurl which is now fixed.
  Some workarounds have been added to libmetha to prevent the freezes from
  occuring when using the defect libcurl versions aswell.  
* Support for older libcurl versions 7.17.x and 7.16.x 
* New information is available in the "this" object of javascript parsers,
  content-type and transfer status code. Read more at
  http://bithack.se/projects/methabot/docs/e4x/this.html 
* --verbose option replaced with --silent, since verbose mode is now default 
* Initial support for FTP crawling and the ftp_dir_url crawler option 
* Depth limiting is now crawler-specific 
* Added the command line options --crawler and --filetype 
* Support for extending and overriding already defined crawlers and filetypes 
* Support for the copy keyword in configuration files 
* Support for dynamically switching the active crawler, this lets you crawl
  different websites in completely different ways in one crawling session. Read
  more about crawler switching at
  http://bithack.se/projects/methabot/docs/crawler_switching.html 
* libev version upgrade to 3.51 
* The include directive in configuration files now makes sure the included
  configuration file hasn't already been loaded, to prevent include-loops and
  multiple filetype/crawler definitions. 
* Various SpiderMonkey garbage collection fixes, libmetha does not crash anymore
  when cleaning up after a multithreaded session 
* Added some extra information to the --info option 
* The 'external' option is now fixed and enabled again 
* New option --spread-workers 
* New libmetha API function lmetha_global_setopt() allows changing the global
  error/message/warning reporter 
* Added initial implementation of a test suite for developers 
* Better error reporting when loading configuration files 
* Bugfix when an HTTP server didn't return a Content-Type header after a HEAD
  request 
* Bugfix when sorting URLs after multiple HTTP HEAD requests 
* Bugfix in the html to xml converter when the HTML page did not have an <html>
  tag 
* Bugfix, the extless-url option did not work 
* Bugfix, html to xml converter no longer chokes on byte-order marks or other
  text before the actual HTML  
* Bugfix, prevented libmetha from trying to access URLs of protocols that are
  not supported 
* Bugfix when shutting down after an error. 
* Bugfix, unresolvable URLs did not break out the retry loop after three retries
* Very experimental and unstable support for Win32, mainly intended for
  developers 
 
New configuration files: 
* google.conf, to perform google searches 
* youtube.conf, youtube searching 
* meta.conf, prints meta information such as keywords and description about HTML
  pages 
* title.conf, prints the title of HTML pages 
* ftp.conf, for crawling FTP servers 
 
Note: Support for crawling local filesystem is currently not available as we
need to reconsider its design and how to integrate it with the parser and
filetype system. 
 
Check out the project website and wiki at http://bithack.se/projects/methabot/)

1.4.1
=====

Time for a bugfix release. This release fixes lots of build-time errors related
to SpiderMonkey on various systems. 
 
Changes: 
* Configure could not find jsapi.h on some systems, this should be fixed now. 
* Configuration files are now able to modify crawler and filetype flags, added
  the options 'external' and 'external_peek' 
* Bugfix, Methabot would sometimes crash when cleaning up empty URLs after
  multiple HTTP HEAD 
* Fixed a crash that occurred when running synchronously. 
* Build system include fix when jsconfig.h could not be found. 

1.4.0
=====

After a long time of hardcore programming, Methabot/1.4.0 is finally ready for
release. You will need libcurl and spidermonkey installed on your system to be
able to compile Methabot. 
 
New features: 
* Completely new architectural design 
* Filetype parser scripting through Javascript/E4X 
* Multithreading is now a primary concept 
* HTTP HEAD requests are now done asynchronously in a separate thread using curl
  and libev 
* Support for "peeking" at external URLs 
* The Methabot Project has been split up into several subprojects, primarily
  there's the command line tool, which uses the web crawling library libmetha as
  its backend.  
* Initial work on the distributed web crawling system Methanol. 
 
So what to do now? Pick up a tutorial on Javascript with E4X and get started
coding your own functions for extracting data from the web. 
 
Check out the brand new Wiki at http://bithack.se/projects/methabot/ for more
information! 

